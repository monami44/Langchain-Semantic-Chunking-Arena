
\documentclass[12pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{float}

\title{Benchmarking Langchain Semantic Chunking Methods: A Comparative Analysis}
\author{}
\date{}

\begin{document}

\maketitle

\begin{abstract}
This study presents a benchmarking environment designed to evaluate and compare four semantic chunking methods provided by Langchain: percentile, interquartile, gradient, and standard deviation. Utilizing diverse datasets comprising 100 abstracts from arXiv machine learning research papers, 100 introductions from PubMed articles on COVID-19, 100 historical documents about World War II, 100 papers on judicial review in European legal systems, and 100 research papers about e-commerce, the benchmarking framework assesses both chunk sizes and retrieval quality against generated ground truths. Metrics such as cosine similarity, precision, recall, F1-score, average precision, and normalized discounted cumulative gain (NDCG) are employed to evaluate performance. The results indicate variations in chunking effectiveness across different methods, providing insights into their suitability for semantic text segmentation tasks across various domains.
\end{abstract}

\section{Introduction}
Semantic chunking is a critical process in natural language processing (NLP) that involves dividing text into meaningful units or "chunks" to facilitate downstream tasks such as information retrieval, summarization, and question answering. Langchain, a popular NLP library, offers several methods for semantic chunking, including percentile, interquartile, gradient, and standard deviation-based approaches. Despite their availability, detailed comparative analyses of these methods are scarce.

This study aims to fill this gap by creating a benchmarking environment to evaluate these chunking methods systematically. By understanding their differences and assessing their performance using well-defined metrics, users can make informed decisions on which method to employ for specific applications.

\section{Methodology}
\subsection{Benchmarking Environment Overview}
The benchmarking environment is designed to:
\begin{enumerate}
    \item \textbf{Evaluate Chunk Sizes}: Analyze the distribution of chunk sizes generated by each method.
    \item \textbf{Assess Retrieval Quality}: Measure how well the chunks facilitate the retrieval of relevant information compared to ground truths.
    \item \textbf{Rank Methods}: Provide a scoring system to rank the chunking methods based on their performance in chunk size distribution and retrieval quality.
\end{enumerate}

\subsection{Datasets}
Five diverse datasets were used:
\begin{itemize}
    \item \textbf{Machine Learning Domain (arXiv)}: 100 abstracts from arXiv research papers on machine learning
    \item \textbf{Medical Domain (PubMed)}: 100 introductions from PubMed articles on COVID-19
    \item \textbf{Historical Domain}: 100 documents about World War II
    \item \textbf{Legal Domain}: 100 papers on judicial review in European legal systems
    \item \textbf{E-commerce Domain}: 100 research papers about e-commerce trends and analysis
\end{itemize}

\subsection{Ground Truth Generation}
Ground truths were generated using the following process:
\begin{enumerate}
    \item \textbf{Question Generation}: For each document, a question was generated using GPT-4, aiming to cover key points discussed in the text.
    \item \textbf{Relevant Chunk Extraction}: GPT-4 was then used to extract the most relevant excerpts (chunks) from the text that answer the generated question.
    \item \textbf{Metadata Association}: Both chunks and ground truths were associated with metadata referencing the original text file, serving as a unique identifier to facilitate accurate mapping.
\end{enumerate}

\subsection{Chunking Methods Implemented}
\begin{enumerate}
    \item \textbf{Percentile Chunker}: Splits text based on specified percentiles of sentence lengths.
    \item \textbf{Interquartile Chunker}: Uses the interquartile range of sentence lengths for chunking.
    \item \textbf{Gradient Chunker}: Divides text based on gradient-based similarity thresholds.
    \item \textbf{Standard Deviation Chunker}: Splits text based on the standard deviation of sentence lengths.
\end{enumerate}

\subsection{Embeddings and Similarity Calculation}
\begin{itemize}
    \item \textbf{Embeddings}: Hugging Face's \texttt{all-MiniLM-L6-v2} model was used to generate embeddings for both chunks and queries.
    \item \textbf{Similarity Measure}: Cosine similarity was computed between query embeddings and chunk embeddings to assess relevance.
\end{itemize}

\subsection{Evaluation Metrics}
\begin{itemize}
    \item \textbf{Chunk Size Metrics}:
    \begin{itemize}
        \item Mean Size
        \item Median Size
        \item Standard Deviation
        \item Minimum and Maximum Sizes
    \end{itemize}
    \item \textbf{Retrieval Quality Metrics}:
    \begin{itemize}
        \item Precision
        \item Recall
        \item F1-Score
        \item Average Precision (AP)
        \item Normalized Discounted Cumulative Gain (NDCG)
    \end{itemize}
\end{itemize}

\subsection{Scoring System}
A weighted scoring system was used to rank the chunking methods:
\begin{itemize}
    \item \textbf{Size Score (40\%)}:
    \begin{itemize}
        \item Mean Score (35\%)
        \item Standard Deviation Score (35\%)
        \item Minimum Size Score (15\%)
        \item Maximum Size Score (15\%)
    \end{itemize}
    \item \textbf{Retrieval Score (60\%)}:
    \begin{itemize}
        \item Precision (20\%)
        \item Recall (20\%)
        \item F1-Score (20\%)
        \item Average Precision (20\%)
        \item NDCG (20\%)
    \end{itemize}
\end{itemize}

\section{Results}
\subsection{Chunk Size Evaluation}
Comprehensive metrics for chunk sizes are provided in Table~\ref{tab:chunk-size}.

\begin{table}[H]
\centering
\caption{Chunk Size Metrics Across Domains and Methods}
\begin{tabular}{|l|l|c|c|c|c|c|}
\hline
Domain & Method & Mean Size & Median Size & Std Dev & Min Size & Max Size \\
\hline
Machine Learning & Gradient & 513.75 & 482.0 & 388.12 & 31 & 1,687 \\
\hline
... & ... & ... & ... & ... & ... & ... \\
\hline
\end{tabular}
\label{tab:chunk-size}
\end{table}

\section{Discussion}
The analysis reveals that the Standard Deviation Chunker performed consistently well across all domains, but domain-specific characteristics should guide the choice of chunking method.

\section{Conclusion}
This benchmarking study highlights the strengths and weaknesses of different chunking methods provided by Langchain. While the Standard Deviation method is robust across multiple domains, tailoring chunking strategies to specific domains can further optimize results.

\end{document}
