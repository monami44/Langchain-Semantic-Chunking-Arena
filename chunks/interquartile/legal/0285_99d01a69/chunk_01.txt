In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in AI and machine learning. Connecting this work to existing legal nondiscrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU nondiscrimination law and jurisprudence of the European Court of Justice ECJ and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the contextsensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ we refer to this approach as contextual equality.This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU nondiscrimination law. Due to the disparate nature of algorithmic and human discrimination, the EUs current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a casebycase basis. We show that automating fairness or nondiscrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems.Second, we show how the legal protection offered by nondiscrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes e.g.